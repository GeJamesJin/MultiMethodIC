import os
import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.kernel_approximation import RBFSampler
from sklearn.metrics import classification_report, accuracy_score


def prepare_data_with_pca_and_rbf(data_path, n_components=100, gamma=0.1):
    """
    Loads and prepares the dataset with PCA and RBF Kernel Transformation.
    """
    dataset = np.load(data_path)
    X, y = dataset["images"], dataset["labels"].flatten()
    X = X.reshape((X.shape[0], -1))  # Flatten the images

    # Apply PCA to reduce dimensionality
    print(f"Applying PCA to reduce dimensions to {n_components} components...")
    pca = PCA(n_components=n_components)
    X_reduced = pca.fit_transform(X)

    # Apply RBF Kernel Transformation
    print(f"Applying RBF Kernel Transformation with gamma={gamma}...")
    rbf_sampler = RBFSampler(gamma=gamma, random_state=42)
    X_rbf = rbf_sampler.fit_transform(X_reduced)

    print(f"Shape after PCA: {X_reduced.shape}, Shape after RBF Kernel Transformation: {X_rbf.shape}")
    return train_test_split(X_rbf, y, test_size=0.2, random_state=42, stratify=y)


def run_grid_search_logistic(X_train, y_train, results_path):
    """
    Performs grid search for Logistic Regression and saves detailed metrics.
    """
    log_reg = LogisticRegression(max_iter=5000)
    param_grid = {
        "C": [0.01, 0.1, 1, 10, 100],
        "solver": ["lbfgs", "liblinear"]
    }
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(log_reg, param_grid, scoring="accuracy", cv=cv, n_jobs=-1, verbose=4, return_train_score=True)
    grid_search.fit(X_train, y_train)

    # Save grid search results
    results_df = pd.DataFrame(grid_search.cv_results_)
    results_file = os.path.join(results_path, "logistic_regression_grid_search_results.csv")
    results_df.to_csv(results_file, index=False)
    print(f"Grid search results saved to: {results_file}")

    # Extract additional metrics
    best_params = grid_search.best_params_
    best_index = grid_search.best_index_
    mean_train_score = grid_search.cv_results_['mean_train_score'][best_index]
    mean_test_score = grid_search.cv_results_['mean_test_score'][best_index]
    mean_fit_time = grid_search.cv_results_['mean_fit_time'][best_index]

    # Save best model metrics
    metrics_file = os.path.join(results_path, "logistic_regression_best_metrics.txt")
    with open(metrics_file, "w") as f:
        f.write(f"Best Parameters: {best_params}\n")
        f.write(f"Best Train Score: {mean_train_score:.4f}\n")
        f.write(f"Best Test Score: {mean_test_score:.4f}\n")
        f.write(f"Mean Fit Time: {mean_fit_time:.4f} seconds\n")
    print(f"Best model metrics saved to: {metrics_file}")

    return grid_search


def train_and_save_logistic(best_model, X_train, y_train, X_test, y_test, save_path):
    """
    Trains the Logistic Regression model with the best parameters and saves it.
    """
    # Train the model
    best_model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=[f"Class {i}" for i in range(10)])

    # Save the model
    model_file = os.path.join(save_path, "logistic_regression.joblib")
    pd.to_pickle(best_model, model_file)
    print(f"Model saved to: {model_file}")

    # Save evaluation results
    eval_file = os.path.join(save_path, "logistic_regression_evaluation.txt")
    with open(eval_file, "w") as f:
        f.write(f"Accuracy: {accuracy:.4f}\n")
        f.write(report)
    print(f"Evaluation results saved to: {eval_file}")


if __name__ == "__main__":
    # Path to the dataset
    train_data_path = os.path.join("train_data", "collected_images.npz")
    results_path = "results/logistic_regression"
    os.makedirs(results_path, exist_ok=True)

    # Prepare data with PCA and RBF Kernel Transformation
    print("Preparing data...")
    X_train, X_test, y_train, y_test = prepare_data_with_pca_and_rbf(
        train_data_path, n_components=100, gamma=0.1
    )

    # Standardize the data
    print("Standardizing the data...")
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Run Grid Search for Logistic Regression
    print("Running Grid Search for Logistic Regression...")
    grid_search = run_grid_search_logistic(X_train, y_train, results_path)

    # Train and Save the Best Model
    print("Training and saving the best model...")
    best_logistic_model = LogisticRegression(**grid_search.best_params_, max_iter=5000)
    train_and_save_logistic(best_logistic_model, X_train, y_train, X_test, y_test, results_path)
